{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D CNN classification of images of raw spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import librosa\n",
    "import numpy as np\n",
    "import librosa.display\n",
    "import errno\n",
    "from scipy.misc import imsave\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Input\n",
    "import time\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from sklearn.manifold import TSNE\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "nb_class = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train_folder_images = 'spectrograms/train/fold4/'\n",
    "target_validation_folder_images = 'spectrograms/validation/fold4/'\n",
    "\n",
    "rows, cols = 257, 313\n",
    "SR = 16000\n",
    "N_FFT = 512\n",
    "HOP_LEN = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if folder path exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sure_path_exists(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exception:\n",
    "        if exception.errno != errno.EEXIST:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wavs = 'audio/train/fold4/'\n",
    "validation_wavs = 'audio/validation/fold4/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training audio to spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for path, subdirs, files in os.walk(train_wavs):\n",
    "    path = path.replace(train_wavs, \"\")\n",
    "    print(\"The total length of audio files in folder:\", path, \"is:\", len(files))\n",
    "    make_sure_path_exists(target_train_folder_images + \"/\" + path)\n",
    "\n",
    "    for audio_name in tqdm(files):\n",
    "        data, rate = librosa.load(train_wavs + \"/\" + path + \"/\" + audio_name, mono=True, sr=SR)\n",
    "\n",
    "        X = librosa.stft(data, n_fft=N_FFT, hop_length=HOP_LEN)\n",
    "        D = librosa.amplitude_to_db(np.abs(X))\n",
    "\n",
    "        D = np.flipud(D)\n",
    "\n",
    "#         plt.imshow(D, cmap='gray')\n",
    "#         plt.show()\n",
    "\n",
    "        if D.shape[0] != rows or D.shape[1] != cols:\n",
    "            print(audio_name, D.shape)\n",
    "            \n",
    "            input(\"wait\")\n",
    "\n",
    "        imsave(target_train_folder_images + \"/\" + path + \"/\" + audio_name.split(\".wav\")[0] + '.png', D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation audio to spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path, subdirs, files in os.walk(validation_wavs):\n",
    "    path = path.replace(validation_wavs, \"\")\n",
    "    print(\"The total length of audio files in folder:\", path, \"is:\", len(files))\n",
    "    make_sure_path_exists(target_validation_folder_images + \"/\" + path)\n",
    "\n",
    "    for audio_name in tqdm(files):\n",
    "        data, rate = librosa.load(validation_wavs + \"/\" + path + \"/\" + audio_name, mono=True, sr=SR)\n",
    "\n",
    "        X = librosa.stft(data, n_fft=N_FFT, hop_length=HOP_LEN)\n",
    "        D = librosa.amplitude_to_db(np.abs(X))\n",
    "\n",
    "        D = np.flipud(D)\n",
    "\n",
    "        # plt.imshow(D, cmap='gray')\n",
    "        # plt.show()\n",
    "\n",
    "        if D.shape[0] != rows or D.shape[1] != cols:\n",
    "            print(audio_name, D.shape)\n",
    "\n",
    "        imsave(target_validation_folder_images + \"/\" + path + \"/\" + audio_name.split(\".wav\")[0] + '.png', D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'keras_spectrograms_fold4'\n",
    "best_weights_path = model_name + '.h5'\n",
    "log_path = model_name + '.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = 'val_acc'\n",
    "\n",
    "input_shape = (257, 313, 1)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "es_patience = 8\n",
    "rlr_patience = 5\n",
    "rlr_factor = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## - Train folder summary\n",
      "spectrograms/train/fold4/outdoor 1170\n",
      "spectrograms/train/fold4/vehicle 936\n",
      "spectrograms/train/fold4/indoor 1404\n",
      "\n",
      "########## - Valid folder summary\n",
      "spectrograms/validation/fold4/outdoor 390\n",
      "spectrograms/validation/fold4/vehicle 312\n",
      "spectrograms/validation/fold4/indoor 468\n",
      "\n",
      "########## - Total files 4680\n",
      "########## - Total classes 3\n"
     ]
    }
   ],
   "source": [
    "print(\"########## - Train folder summary\")\n",
    "\n",
    "train_total_files = 0\n",
    "\n",
    "for path, subdirs, files in os.walk(target_train_folder_images):\n",
    "    path = path.replace(target_train_folder_images + \"/\", \"\")\n",
    "    if path != target_train_folder_images:\n",
    "\n",
    "        print(path, len(files))\n",
    "        train_total_files += len(files)\n",
    "\n",
    "print()\n",
    "print(\"########## - Valid folder summary\")\n",
    "\n",
    "valid_total_files = 0\n",
    "\n",
    "for path, subdirs, files in os.walk(target_validation_folder_images):\n",
    "    path = path.replace(target_validation_folder_images + \"/\", \"\")\n",
    "    if path != target_validation_folder_images:\n",
    "\n",
    "        print(path, len(files))\n",
    "        valid_total_files += len(files)\n",
    "\n",
    "print()\n",
    "print(\"########## - Total files\", train_total_files + valid_total_files)\n",
    "print(\"########## - Total classes\", nb_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########## - Train folder\n",
      "Found 3510 images belonging to 3 classes.\n",
      "########## - Valid folder\n",
      "Found 1170 images belonging to 3 classes.\n",
      "{'vehicle': 2, 'outdoor': 1, 'indoor': 0}\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "print(\"\\n########## - Train folder\")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    target_train_folder_images,\n",
    "    target_size=(input_shape[0], input_shape[1]),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale',\n",
    "    shuffle=True)\n",
    "\n",
    "print(\"########## - Valid folder\")\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    target_validation_folder_images,\n",
    "    target_size=(input_shape[0], input_shape[1]),\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale',\n",
    "    shuffle=False)\n",
    "\n",
    "print(valid_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "def sklearn_f1(label, pred):\n",
    "    final_preds = []\n",
    "\n",
    "    for i in range(len(pred)):\n",
    "        final_preds.append(np.argmax(pred[i]))\n",
    "\n",
    "    final_preds = np.array(final_preds)\n",
    "\n",
    "    return f1_score(label, final_preds, labels=None, pos_label=1, average='macro', sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_model(input_shape=input_shape, num_classes = 3):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(2, 2), activation='relu', input_shape=input_shape))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(48, kernel_size=(2, 2), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(120, kernel_size=(2, 2), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "#     model.add(Dense(128, activation='relu'))\n",
    "#     model.add(Dropout(0.25))\n",
    "#     model.add(Dense(64, activation='relu'))\n",
    "#     model.add(Dropout(0.4))\n",
    "    model.add(Dense(num_classes, activation='softmax', name=\"output_layer\"))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adam(),\n",
    "                  metrics=['accuracy', f1])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 256, 312, 32)      160       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 255, 311, 48)      6192      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 127, 155, 48)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 126, 154, 120)     23160     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 63, 77, 120)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 582120)            0         \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 3)                 1746363   \n",
      "=================================================================\n",
      "Total params: 1,775,875\n",
      "Trainable params: 1,775,875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# img_input = Input(shape=input_shape)\n",
    "\n",
    "model = construct_model(input_shape=input_shape, num_classes=nb_class)\n",
    "\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', f1])\n",
    "##############################################################################\n",
    "\n",
    "callbacks_list = [ModelCheckpoint(monitor=monitor,\n",
    "                                  filepath=best_weights_path,\n",
    "                                  save_best_only=True,\n",
    "                                  save_weights_only=True,\n",
    "                                  mode='max',\n",
    "                                  verbose=1),\n",
    "                  EarlyStopping(monitor=monitor,\n",
    "                                patience=es_patience,\n",
    "                                verbose=1),\n",
    "                  ReduceLROnPlateau(monitor=monitor,\n",
    "                                    factor=rlr_factor,\n",
    "                                    patience=rlr_patience,\n",
    "                                    verbose=1),\n",
    "                  CSVLogger(filename=log_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########## - TRAINING\n",
      "\n",
      "Epoch 1/50\n",
      "110/110 [==============================] - 42s 381ms/step - loss: 0.8365 - acc: 0.5980 - f1: 0.5545 - val_loss: 0.6901 - val_acc: 0.7179 - val_f1: 0.6596\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.71795, saving model to keras_spectrograms_fold4.h5\n",
      "Epoch 2/50\n",
      "110/110 [==============================] - 11s 104ms/step - loss: 0.6215 - acc: 0.7356 - f1: 0.7211 - val_loss: 0.5998 - val_acc: 0.7701 - val_f1: 0.7639\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.71795 to 0.77009, saving model to keras_spectrograms_fold4.h5\n",
      "Epoch 3/50\n",
      "110/110 [==============================] - 11s 104ms/step - loss: 0.4722 - acc: 0.8283 - f1: 0.8249 - val_loss: 0.4767 - val_acc: 0.8333 - val_f1: 0.8337\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.77009 to 0.83333, saving model to keras_spectrograms_fold4.h5\n",
      "Epoch 4/50\n",
      "110/110 [==============================] - 11s 104ms/step - loss: 0.3553 - acc: 0.8740 - f1: 0.8716 - val_loss: 0.4245 - val_acc: 0.8385 - val_f1: 0.8357\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.83333 to 0.83846, saving model to keras_spectrograms_fold4.h5\n",
      "Epoch 5/50\n",
      "110/110 [==============================] - 11s 104ms/step - loss: 0.3093 - acc: 0.8870 - f1: 0.8833 - val_loss: 0.4135 - val_acc: 0.8419 - val_f1: 0.8354\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.83846 to 0.84188, saving model to keras_spectrograms_fold4.h5\n",
      "Epoch 6/50\n",
      "110/110 [==============================] - 11s 104ms/step - loss: 0.2319 - acc: 0.9175 - f1: 0.9172 - val_loss: 0.5569 - val_acc: 0.8205 - val_f1: 0.8148\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.84188\n",
      "Epoch 7/50\n",
      "110/110 [==============================] - 11s 104ms/step - loss: 0.1810 - acc: 0.9384 - f1: 0.9394 - val_loss: 0.3793 - val_acc: 0.8872 - val_f1: 0.8836\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.84188 to 0.88718, saving model to keras_spectrograms_fold4.h5\n",
      "Epoch 8/50\n",
      "110/110 [==============================] - 11s 104ms/step - loss: 0.1476 - acc: 0.9487 - f1: 0.9476 - val_loss: 0.3961 - val_acc: 0.8889 - val_f1: 0.8883\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.88718 to 0.88889, saving model to keras_spectrograms_fold4.h5\n",
      "Epoch 9/50\n",
      "110/110 [==============================] - 11s 104ms/step - loss: 0.0825 - acc: 0.9745 - f1: 0.9747 - val_loss: 0.3726 - val_acc: 0.8855 - val_f1: 0.8848\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.88889\n",
      "Epoch 10/50\n",
      "110/110 [==============================] - 11s 104ms/step - loss: 0.0496 - acc: 0.9857 - f1: 0.9852 - val_loss: 0.4725 - val_acc: 0.8821 - val_f1: 0.8823\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.88889\n",
      "Epoch 11/50\n",
      "110/110 [==============================] - 11s 104ms/step - loss: 0.0498 - acc: 0.9866 - f1: 0.9865 - val_loss: 0.7452 - val_acc: 0.8205 - val_f1: 0.8183\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.88889\n",
      "Epoch 12/50\n",
      "110/110 [==============================] - 11s 104ms/step - loss: 0.0254 - acc: 0.9926 - f1: 0.9926 - val_loss: 0.5948 - val_acc: 0.8675 - val_f1: 0.8686\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.88889\n",
      "Epoch 13/50\n",
      "110/110 [==============================] - 12s 105ms/step - loss: 0.0155 - acc: 0.9974 - f1: 0.9974 - val_loss: 0.6022 - val_acc: 0.8513 - val_f1: 0.8507\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.88889\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 14/50\n",
      "110/110 [==============================] - 11s 104ms/step - loss: 0.0058 - acc: 0.9994 - f1: 0.9994 - val_loss: 0.4661 - val_acc: 0.8838 - val_f1: 0.8832\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.88889\n",
      "Epoch 15/50\n",
      "110/110 [==============================] - 12s 105ms/step - loss: 0.0035 - acc: 1.0000 - f1: 1.0000 - val_loss: 0.4650 - val_acc: 0.8880 - val_f1: 0.8867\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.88889\n",
      "Epoch 16/50\n",
      " 96/110 [=========================>....] - ETA: 1s - loss: 0.0029 - acc: 1.0000 - f1: 1.0000"
     ]
    }
   ],
   "source": [
    "print(\"\\n########## - TRAINING\\n\")\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                              steps_per_epoch=int(math.ceil(float(train_total_files) / float(batch_size))),\n",
    "                              validation_data=valid_generator,\n",
    "                              validation_steps=int(math.ceil(float(valid_total_files) / float(batch_size))),\n",
    "                              epochs=epochs,\n",
    "                              callbacks=callbacks_list,\n",
    "                              shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training accuracy: %.2f%% / Validation accuracy: %.2f%%\" % \n",
    "      (100*history.history['acc'][-1], 100*history.history['val_acc'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_generator.reset()\n",
    "Y_pred = model.predict_generator(valid_generator, valid_total_files // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "# print('Confusion Matrix')\n",
    "# print(confusion_matrix(valid_generator.classes, y_pred))\n",
    "# print('Classification Report')\n",
    "target_names = ['indoor', 'outdoor', 'vehicle']\n",
    "# print(classification_report(valid_generator.classes, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(valid_generator.classes, y_pred), \n",
    "            annot=True, fmt=\"d\", xticklabels=target_names, yticklabels=target_names, cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", accuracy_score(valid_generator.classes, y_pred))\n",
    "print(\"F1 Score: \", f1_score(valid_generator.classes, y_pred, average=\"macro\"))\n",
    "print(\"Precision Score: \", precision_score(valid_generator.classes, y_pred, average=\"macro\"))\n",
    "print(\"Recall Score: \", recall_score(valid_generator.classes, y_pred, average=\"macro\")) \n",
    "print(classification_report(valid_generator.classes, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
